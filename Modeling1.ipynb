{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add96f3e-1112-40fa-81c7-79b989c21c46",
   "metadata": {},
   "source": [
    "### Biosynthetic Gene Clusters (BGCs) design using Attention-Based Conditional Variational Autoencoder (cVAE)\n",
    "\n",
    "The model is conditioned on the class of the BGC to which the protein or domain belongs (e.g., Class 1, Class 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e02026f-27e4-40c5-aff8-07a4e3cdd086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.83\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "#Loading packages\n",
    "#Importing packages\n",
    "\n",
    "import Bio\n",
    "print(Bio.__version__)\n",
    "from Bio import SeqIO  # Import SeqIO from the Bio package\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.linalg import sqrtm\n",
    "import time\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40753607-d7b9-44f2-9042-52f9f7d8fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "Data Shape: (19450, 20372)\n",
      "Sample Data: [ 0.          0.          0.         ... -0.08841293 -0.02668052\n",
      "  0.07184622]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the embeddings file\n",
    "file_path = \"/Users/josephtsenum/Documents/PHA6935_AI_for_Drug_Discovery/Project/hybrid_pfam_esm_embeddings.npy\"\n",
    "\n",
    "# Load the embeddings\n",
    "integrated_embeddings = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "# Inspect the loaded data\n",
    "print(\"Data Type:\", type(integrated_embeddings))\n",
    "print(\"Data Shape:\", integrated_embeddings.shape if hasattr(integrated_embeddings, 'shape') else \"No shape attribute\")\n",
    "print(\"Sample Data:\", integrated_embeddings[0] if isinstance(integrated_embeddings, (list, np.ndarray)) else integrated_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fa7725-4cd3-417e-9312-5f884fb6923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Shape: torch.Size([19450, 20372])\n"
     ]
    }
   ],
   "source": [
    "### Data preparation\n",
    "\n",
    "import torch\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "embedding_data = torch.tensor(integrated_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Inspect the tensor\n",
    "print(\"Tensor Shape:\", embedding_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e5c38-f4d5-4378-b2aa-335d981490c7",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Concatenate the condition (e.g., BGC class) with the input embeddings. This ensures the latent space is informed by the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae7cada-7af1-4e5f-893b-bf94c755ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 1D CNN layers for feature extraction\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers for latent space\n",
    "        self.fc1 = nn.Linear(32 * input_dim + condition_dim, 1024)\n",
    "        self.fc2_mean = nn.Linear(1024, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(1024, latent_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        \n",
    "        # Expand dimensions for 1D CNN: (batch_size, 1, input_dim)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Pass through 1D CNN layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten the output for fully connected layers\n",
    "        x = self.flatten(x)  # Shape: (batch_size, 32 * input_dim)\n",
    "        \n",
    "        # Concatenate input embeddings with condition vector\n",
    "        x = torch.cat((x, condition), dim=1)  # Shape: (batch_size, 32 * input_dim + condition_dim)\n",
    "        \n",
    "        # Pass through fully connected layers to get mean and logvar\n",
    "        x = self.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b18e7-287a-476b-8521-ea0c2b9fe471",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Concatenate the condition with the latent vector before decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7816c72-5b63-4920-93c2-ba90c8fa5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, output_dim, sequence_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # RNN Layer (LSTM)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=latent_dim + condition_dim, \n",
    "            hidden_size=latent_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc_output = nn.Linear(latent_dim, output_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Sequence length for reconstructing the input\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        # Concatenate latent vector and condition\n",
    "        z_c = torch.cat((z, condition), dim=1)  # Shape: (batch_size, latent_dim + condition_dim)\n",
    "        \n",
    "        # Expand to match sequence length: (batch_size, sequence_length, latent_dim + condition_dim)\n",
    "        z_c = z_c.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        rnn_output, _ = self.rnn(z_c)  # Shape: (batch_size, sequence_length, latent_dim)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.attention(rnn_output, rnn_output, rnn_output)  # Self-attention: (batch_size, sequence_length, latent_dim)\n",
    "        \n",
    "        # Map attention output to embedding space\n",
    "        output = self.sigmoid(self.fc_output(attn_output))  # Shape: (batch_size, sequence_length, output_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876afc24-2ed5-4a39-9a31-b8435a92439f",
   "metadata": {},
   "source": [
    "### Updating the cVAE Class\n",
    "\n",
    "Pass the condition into both the encoder and decoder during the forward pass. Updated cVAE class structure with the self-attention-enabled decoder and the 1D CNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b4816a-5680-420b-bd78-88f43f5b96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, sequence_length):\n",
    "        super(cVAE, self).__init__()\n",
    "        # Encoder: 1D CNN-based Encoder\n",
    "        self.encoder = Encoder(input_dim, condition_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: RNN with self-attention\n",
    "        self.decoder = Decoder(latent_dim, condition_dim, output_dim, sequence_length)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Encode the input to obtain mean and log variance\n",
    "        mean, logvar = self.encoder(x, condition)\n",
    "        \n",
    "        # Reparameterization trick: sample latent vector z\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mean + std * torch.randn_like(std)\n",
    "        \n",
    "        # Decode the latent vector\n",
    "        reconstructed = self.decoder(z, condition)\n",
    "        \n",
    "        return reconstructed, mean, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da6323-797c-4b54-90bc-92a81ef72068",
   "metadata": {},
   "source": [
    "### Preparing the Conditioning Variable\n",
    "\n",
    "Since our condition is categorical (BGC class), we will one-hot encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473574f0-4985-438a-9353-24c770f29592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated BGC Classes Length: 19450\n",
      "Sample BGC Classes: ['Class1', 'Class2', 'Class3', 'Class1', 'Class2', 'Class3', 'Class1', 'Class2', 'Class3', 'Class1']\n"
     ]
    }
   ],
   "source": [
    "# Ensure the number of BGC classes matches the number of rows in embedding_data\n",
    "num_samples = embedding_data.shape[0]  # Number of rows in embedding_data\n",
    "\n",
    "# Define your BGC class labels (you can adjust the actual classes as needed)\n",
    "bgc_classes = ['Class1', 'Class2', 'Class3'] * (num_samples // 3) + ['Class1'] * (num_samples % 3)\n",
    "\n",
    "# Verify the length of bgc_classes matches num_samples\n",
    "assert len(bgc_classes) == num_samples, \"bgc_classes length does not match embedding_data\"\n",
    "\n",
    "print(\"Updated BGC Classes Length:\", len(bgc_classes))\n",
    "print(\"Sample BGC Classes:\", bgc_classes[:10])  # Display a sample of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666910e4-94e4-4108-89f4-145d51f68d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Data Shape: torch.Size([19450, 3])\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncode the corrected bgc_classes\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Create OneHotEncoder instance\n",
    "bgc_classes_array = np.array(bgc_classes).reshape(-1, 1)  # Reshape for encoding\n",
    "condition_data = torch.tensor(encoder.fit_transform(bgc_classes_array), dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# Verify the shape of condition_data\n",
    "assert condition_data.shape[0] == embedding_data.shape[0], \"Condition data shape mismatch\"\n",
    "print(\"Condition Data Shape:\", condition_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a86da5-3240-446a-afbc-47e268aa28a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Data Shape: torch.Size([19450, 20372])\n",
      "Number of BGC Classes: 19450\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding Data Shape:\", embedding_data.shape)\n",
    "print(\"Number of BGC Classes:\", len(bgc_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c99c35-d4f4-44da-a08d-94638dd6eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Data Shape: torch.Size([19450, 3])\n"
     ]
    }
   ],
   "source": [
    "# Number of embeddings\n",
    "num_embeddings = embedding_data.shape[0]\n",
    "\n",
    "# Define the unique BGC classes\n",
    "classes = ['Class1', 'Class2', 'Class3']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Create the list of BGC class labels by cycling through the classes\n",
    "bgc_classes = [classes[i % num_classes] for i in range(num_embeddings)]\n",
    "\n",
    "# One-hot encode the BGC classes\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Create the encoder\n",
    "condition_data = torch.tensor(\n",
    "    encoder.fit_transform(np.array(bgc_classes).reshape(-1, 1)),  # One-hot encoding\n",
    "    dtype=torch.float32  # Convert to PyTorch tensor\n",
    ")\n",
    "\n",
    "# Verify that the condition data matches the embedding data\n",
    "assert condition_data.shape[0] == embedding_data.shape[0], \"Mismatch between embeddings and conditions\"\n",
    "print(\"Condition Data Shape:\", condition_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee093915-42d9-40fe-be37-29c8747ad06f",
   "metadata": {},
   "source": [
    "### Updating Training Loop\n",
    "\n",
    "Pass the condition into the forward pass of the cVAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79aae92-2f5e-4db1-ba73-53ab60ae62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Data Shape: torch.Size([19450, 20372])\n",
      "Condition Data Shape: torch.Size([19450, 3])\n"
     ]
    }
   ],
   "source": [
    "# Reload embedding_data and condition_data if necessary\n",
    "if isinstance(embedding_data, np.ndarray):\n",
    "    embedding_data = torch.tensor(embedding_data, dtype=torch.float32)\n",
    "if isinstance(condition_data, np.ndarray):\n",
    "    condition_data = torch.tensor(condition_data, dtype=torch.float32)\n",
    "\n",
    "# Ensure shapes are correct\n",
    "print(f\"Embedding Data Shape: {embedding_data.shape}\")\n",
    "print(f\"Condition Data Shape: {condition_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030a6d-e511-48e8-ac9e-ac16d49c1233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b995cf-210a-46fd-85d8-7f311d3c07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated with input_dim=20372, condition_dim=3, latent_dim=256, output_dim=20372\n"
     ]
    }
   ],
   "source": [
    "# Define input, latent, and output dimensions\n",
    "input_dim = embedding_data.shape[1]  # Number of features in embedding_data\n",
    "latent_dim = 256  # Dimension of latent space\n",
    "output_dim = input_dim  # Same as input_dim for reconstruction task\n",
    "condition_dim = condition_data.shape[1]  # Dimension of the condition vector\n",
    "\n",
    "# Instantiate the cVAE\n",
    "vae = cVAE(input_dim, condition_dim, latent_dim, output_dim, sequence_length=input_dim)\n",
    "print(f\"Model instantiated with input_dim={input_dim}, condition_dim={condition_dim}, latent_dim={latent_dim}, output_dim={output_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acab5a-f368-4d44-8ebf-3cbe5591d963",
   "metadata": {},
   "source": [
    "### Defining loss_function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0af3fa-61da-48ea-a9be-89062097d5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and optimizer defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "def loss_function(reconstructed, original, mean, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    reconstruction_loss = nn.MSELoss()(reconstructed, original)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    \n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Loss function and optimizer defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61295761-7337-42d8-867d-ac5d2e90e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Try a smaller batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3fd249-8481-45e0-90bd-a44159c37fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Batch Size: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current Batch Size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6c773-30d1-4beb-96c1-5bf4b5988647",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76254a50-a5fd-4124-810e-f40a8ccbc86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/hr2vkz5d6rsdcwsjr4bf6ydh0000gn/T/ipykernel_10631/2666723612.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast, GradScaler\n\u001b[1;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     vae\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, embedding_data.shape[0], batch_size):\n",
    "        batch = embedding_data[i:i + batch_size].to(device)\n",
    "        condition_batch = condition_data[i:i + batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Use mixed precision\n",
    "            reconstructed, mean, logvar = vae(batch, condition_batch)\n",
    "            loss = loss_function(reconstructed, batch, mean, logvar)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf74b5-d1a1-4254-8ffd-904b6c4d887c",
   "metadata": {},
   "source": [
    "### Evaluating the Training Loss\n",
    "\n",
    "Plotting the training loss over epochs to understand how well the model converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2a7e9-dde2-4789-967d-de18dacc45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e234e2a-912c-458d-bf5f-0bf4bfe8e3a2",
   "metadata": {},
   "source": [
    "### Visualizing the Latent Space\n",
    "\n",
    "To evaluate the latent representations, we will:\n",
    "\n",
    "1. Encode the input embeddings using the encoder.\n",
    "2. Reduce the latent space to 2D using dimensionality reduction techniques, t-SNE.\n",
    "3. Visualize the points, optionally color-coded by their conditions (BGC class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbf7fe-8067-4005-94dc-479e03627e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass embeddings through the encoder\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    latent_vectors = []\n",
    "    for i in range(embedding_data.shape[0]):\n",
    "        embedding = embedding_data[i].unsqueeze(0)  # Add batch dimension\n",
    "        condition = condition_data[i].unsqueeze(0)  # Add batch dimension\n",
    "        mean, _ = vae.encoder(embedding, condition)\n",
    "        latent_vectors.append(mean.numpy().flatten())\n",
    "\n",
    "latent_vectors = np.array(latent_vectors)\n",
    "\n",
    "# Use t-SNE for dimensionality reduction to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "# Plot the reduced latent space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=np.argmax(condition_data.numpy(), axis=1), cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label=\"Condition (BGC Class)\")\n",
    "plt.title(\"Latent Space Visualization\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cf69d-0880-439e-86b8-a17511b2858e",
   "metadata": {},
   "source": [
    "### Reconstructing Inputs\n",
    "\n",
    "Evaluate how well the decoder can reconstruct the input embeddings given the latent representations and conditions. We will measure reconstruction quality and visualize reconstructed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20963da4-ad7b-4e36-aec8-0b583b7690e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few random samples to reconstruct\n",
    "vae.eval()\n",
    "samples = embedding_data[:5]  # Replace with random indices if needed\n",
    "conditions = condition_data[:5]\n",
    "\n",
    "# Perform reconstruction\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = vae(samples, conditions)\n",
    "\n",
    "# Compare original and reconstructed embeddings\n",
    "for i in range(samples.shape[0]):\n",
    "    print(f\"Sample {i + 1}\")\n",
    "    print(\"Original:\", samples[i].numpy()[:10])  # Show first 10 features\n",
    "    print(\"Reconstructed:\", reconstructed[i].numpy()[:10])  # Show first 10 features\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee9998-a7c8-4d7b-bd48-7449aed0417b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbb3c6-4c4a-46fb-92fe-1bcc3a9e373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "vae.eval()\n",
    "\n",
    "# Select a few random samples to reconstruct\n",
    "num_samples = 5  # Number of samples to visualize\n",
    "sample_indices = torch.randint(0, embedding_data.shape[0], (num_samples,))  # Random indices\n",
    "samples = embedding_data[sample_indices]\n",
    "conditions = condition_data[sample_indices]\n",
    "\n",
    "# Perform reconstruction\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = vae(samples, conditions)\n",
    "\n",
    "# Plot original and reconstructed embeddings for each sample\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot original embedding\n",
    "    plt.plot(samples[i].numpy(), label=\"Original\", alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # Plot reconstructed embedding\n",
    "    plt.plot(reconstructed[i].numpy(), label=\"Reconstructed\", alpha=0.7, linewidth=2, linestyle=\"--\")\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.title(f\"Sample {i + 1}: Original vs Reconstructed Embedding\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ef071-8480-49c2-9474-4f149883a842",
   "metadata": {},
   "source": [
    "### Predict New Domain Configurations\n",
    "\n",
    "To generate new domain configurations or embeddings, we will sample from the latent space and use the decoder.\n",
    "\n",
    "Generate New Configurations:\n",
    "\n",
    "1. Samples latent vectors from a Gaussian distribution (latent_samples).\n",
    "2. Uses the decoder to generate new domain embeddings based on random conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264765b-a2b3-4420-a5e5-10879cc7bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample latent vectors from a Gaussian distribution\n",
    "num_samples = 5\n",
    "latent_samples = torch.randn((num_samples, latent_dim))\n",
    "\n",
    "# Use the decoder with random conditions (e.g., one-hot encoded random BGC class)\n",
    "random_conditions = condition_data[:num_samples]  # Replace with specific conditions if needed\n",
    "generated = vae.decoder(latent_samples, random_conditions)\n",
    "\n",
    "# Print or analyze generated embeddings\n",
    "print(\"Generated Embeddings:\")\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b6135-2073-4e01-99c0-7b8bd69529c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new domain configurations by sampling from the latent space\n",
    "vae.eval()  # Ensure the model is in evaluation mode\n",
    "num_samples = 5  # Number of new samples to generate\n",
    "\n",
    "# Sample latent vectors from a Gaussian distribution\n",
    "latent_samples = torch.randn((num_samples, latent_dim))\n",
    "\n",
    "# Use random conditions for generation\n",
    "random_conditions = condition_data[:num_samples]  # Select the first few conditions\n",
    "\n",
    "# Generate new embeddings using the decoder\n",
    "with torch.no_grad():\n",
    "    generated_embeddings = vae.decoder(latent_samples, random_conditions)\n",
    "\n",
    "# Plot the generated embeddings\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(generated_embeddings[i].numpy(), label=f\"Generated Sample {i + 1}\", alpha=0.7, linewidth=2)\n",
    "    plt.title(f\"Generated Embedding {i + 1}\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135588e7-dad2-4741-9534-be2a54eacf05",
   "metadata": {},
   "source": [
    "### Save Results\n",
    "\n",
    "Saving the trained model, latent representations, or reconstructed outputs for further use or analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcf808-f3cc-44e9-a2a0-8ce2fa8dfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(vae.state_dict(), \"trained_cvae_model_50_epochs.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664e4f0-9ed9-405a-84e4-5892ceb04d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save latent vectors and conditions for further analysis\n",
    "np.save(\"latent_vectors.npy\", latent_vectors)\n",
    "np.save(\"latent_conditions.npy\", condition_data.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e8f45-301d-4a61-b82a-e908d23121e9",
   "metadata": {},
   "source": [
    "### Evaluation with Fréchet Inception Distance (FID) Score\n",
    "\n",
    "Fréchet Inception Distance (FID) is a metric for evaluating generative models like a cVAE, especially for assessing the quality of generated data (e.g., embeddings or domain configurations). The FID score compares the distributions of the real and generated data in a feature space, and lower FID scores indicate that the generated data is closer to the real data.\n",
    "\n",
    "Why FID Score?\n",
    "\n",
    "1. Reconstruction-Based Tasks: FID measures how similar the generated embeddings (or reconstructions) are to the real ones in terms of distribution.\n",
    "2. Robustness: It captures not just pointwise similarity (like MSE) but also the overall quality and diversity of generated samples.\n",
    "3. Widely Used: FID is commonly used in generative modeling tasks (e.g., GANs, VAEs) for evaluating the fidelity and diversity of generated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40363b3c-6601-485b-b253-60f0029221b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = cVAE(input_dim, condition_dim, latent_dim, output_dim)\n",
    "vae.load_state_dict(torch.load(\"trained_cvae_model_50_epochs.pth\"))\n",
    "vae.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436685f7-fb18-4ff5-8694-f02a0d5df592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding Data Shape:\", embedding_data.shape)\n",
    "print(\"Condition Data Shape:\", condition_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a628af5-e81d-4a42-9903-62b3e5f5c732",
   "metadata": {},
   "source": [
    "#### Compute FID Score\n",
    "\n",
    "Extracting Real and Generated Embeddings:\n",
    "1. Pass real embeddings (from embedding_data) and generated embeddings (from the decoder) through a feature extractor (e.g., an Inception model).\n",
    "2. Collect the activations (intermediate feature representations) for both sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baabd3-21b1-4f5b-b5ad-191c25cfbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate reconstructions\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = vae(embedding_data, condition_data)\n",
    "\n",
    "# Use the original embeddings (real) and reconstructed embeddings (generated)\n",
    "real_features = embedding_data.numpy()  # Real embeddings\n",
    "generated_features = reconstructed.numpy()  # Generated embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d4682-9d74-445d-9e8c-b4fc92670fad",
   "metadata": {},
   "source": [
    "### Calculating the Mean and Covariance\n",
    "\n",
    "Compute the mean and covariance for both the real and generated embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9036f-5867-45e9-af79-5dedd1020636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(features):\n",
    "    \"\"\"Calculate mean and covariance of the features.\"\"\"\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=False)\n",
    "    return mean, cov\n",
    "\n",
    "# Compute statistics for real and generated features\n",
    "real_mean, real_cov = calculate_statistics(real_features)\n",
    "gen_mean, gen_cov = calculate_statistics(generated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c976c-d42c-4d24-aadb-d5ee1e92e7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee891cdf-986b-42ac-9739-ccbfeb0b1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the means for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(real_mean, label=\"Real Embeddings Mean\", alpha=0.7, linewidth=2)\n",
    "plt.plot(gen_mean, label=\"Generated Embeddings Mean\", alpha=0.7, linewidth=2, linestyle=\"--\")\n",
    "plt.title(\"Mean of Real vs Generated Embeddings\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d70ae-a653-4db2-9f46-26521685581c",
   "metadata": {},
   "source": [
    "### Computing the FID Score\n",
    "\n",
    "The FID score is calculated as:\n",
    "\n",
    "$$\n",
    "FID = ||\\mu_1 - \\mu_2||^2 + \\text{Tr}(C_1 + C_2 - 2 \\sqrt{C_1 C_2})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **$\\mu_1$**: Mean of the real embeddings (feature distribution of real data).\n",
    "- **$\\mu_2$**: Mean of the generated embeddings (feature distribution of generated data).\n",
    "- **$C_1$**: Covariance matrix of the real embeddings.\n",
    "- **$C_2$**: Covariance matrix of the generated embeddings.\n",
    "- **$\\text{Tr}$**: Trace of a matrix (sum of its diagonal elements).\n",
    "- **$||\\mu_1 - \\mu_2||^2$**: Squared Euclidean distance between the means of the real and generated embeddings.\n",
    "- **$\\sqrt{C_1 C_2}$**: Matrix square root of the product of the covariance matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccfd95-5f8b-43d5-a517-6b63c289bdf7",
   "metadata": {},
   "source": [
    "### Generate Reconstructions\n",
    "Use your trained cVAE model to reconstruct the embeddings based on the input embedding_data and condition_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277e519-54bb-4207-8c02-3c807ae91981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    reconstructed, _, _ = vae(embedding_data, condition_data)  # Generate reconstructed embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4149c-4742-4513-a1e6-a13d4464df33",
   "metadata": {},
   "source": [
    "### Define Real and Generated Features\n",
    "Convert both the original embeddings (embedding_data) and the reconstructed embeddings (reconstructed) to NumPy for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b01d5-0147-4d1b-8a47-18bc223f55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to NumPy\n",
    "real_features = embedding_data.numpy()  # Real embeddings\n",
    "generated_features = reconstructed.numpy()  # Generated embeddings\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Real Features Shape:\", real_features.shape)\n",
    "print(\"Generated Features Shape:\", generated_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129491b7-d541-412f-9f02-f8bd2a47cf6f",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality with PCA\n",
    "Perform PCA to reduce the dimensionality of the real and generated embeddings. This helps make covariance computations faster and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820176a-8530-493d-9ea1-3714d4ec9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=512)  # Adjust the number of components if necessary\n",
    "real_features_reduced = pca.fit_transform(real_features)\n",
    "gen_features_reduced = pca.transform(generated_features)\n",
    "\n",
    "# Verify shapes after PCA\n",
    "print(\"Reduced Real Features Shape:\", real_features_reduced.shape)\n",
    "print(\"Reduced Generated Features Shape:\", gen_features_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe621196-db12-4161-9fc0-ab97ec680874",
   "metadata": {},
   "source": [
    "### Compute Mean and Covariance\n",
    "Calculate the mean and covariance of the reduced embeddings using the previously defined calculate_statistics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9daafd-694c-4132-acc0-2f3469818331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(features):\n",
    "    \"\"\"Calculate mean and covariance of the features.\"\"\"\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=False)\n",
    "    return mean, cov\n",
    "\n",
    "# Compute statistics for real and generated features\n",
    "real_mean, real_cov = calculate_statistics(real_features_reduced)\n",
    "gen_mean, gen_cov = calculate_statistics(gen_features_reduced)\n",
    "\n",
    "# Print the computed statistics\n",
    "print(\"Real Mean Shape:\", real_mean.shape)\n",
    "print(\"Real Covariance Shape:\", real_cov.shape)\n",
    "print(\"Generated Mean Shape:\", gen_mean.shape)\n",
    "print(\"Generated Covariance Shape:\", gen_cov.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5a5e3-0be0-49d2-818c-4abd5e5976f3",
   "metadata": {},
   "source": [
    "### Compute FID Score\n",
    "Using the computed mean and covariance, calculate the FID score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa4678-638f-4aba-bc22-77aa3c08bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_mean, real_cov, gen_mean, gen_cov):\n",
    "    \"\"\"Calculate the Fréchet Inception Distance (FID).\"\"\"\n",
    "    mean_diff = np.sum((real_mean - gen_mean) ** 2)  # Squared Euclidean distance between means\n",
    "    cov_sqrt = sqrtm(real_cov @ gen_cov)  # Matrix square root of product of covariances\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real  # Handle numerical instability\n",
    "    fid = mean_diff + np.trace(real_cov + gen_cov - 2 * cov_sqrt)  # FID formula\n",
    "    return fid\n",
    "\n",
    "# Compute FID score\n",
    "fid_score = calculate_fid(real_mean, real_cov, gen_mean, gen_cov)\n",
    "print(f\"FID Score: {fid_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191ea79-1918-4b3b-b3de-735bd4f00c54",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "Plot the means and covariance diagonal to compare the distributions of real and generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89686159-0c24-4a7f-b61f-2ad7e8f0c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9924ef2-7bd8-42ac-932c-a8c715dc93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the means for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(real_mean, label=\"Real Embeddings Mean\", alpha=0.7, linewidth=2)\n",
    "plt.plot(gen_mean, label=\"Generated Embeddings Mean\", alpha=0.7, linewidth=2, linestyle=\"--\")\n",
    "plt.title(\"Mean of Real vs Generated Embeddings\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57661ff-f720-42b6-a6e7-30b87a1accaf",
   "metadata": {},
   "source": [
    "### Plot Covariance Diagonal Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ff424-5139-4578-a8db-c2f7e9a5191e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df8466-460f-4904-a679-3bc731565c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the diagonal of the covariance matrices for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.diag(real_cov), label=\"Real Embeddings Covariance (Diagonal)\", alpha=0.7, linewidth=2)\n",
    "plt.plot(np.diag(gen_cov), label=\"Generated Embeddings Covariance (Diagonal)\", alpha=0.7, linewidth=2, linestyle=\"--\")\n",
    "plt.title(\"Covariance (Diagonal) of Real vs Generated Embeddings\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Variance Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a35a-761e-4c71-9cb1-6d2ce5d9d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a random sample of embeddings\n",
    "print(\"Sample Embedding:\", embedding_data[0].numpy())\n",
    "print(\"Number of Zeros in Sample Embedding:\", (embedding_data[0] == 0).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d74b8-0297-456a-8e12-adcce69f956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c72432f-f48f-4544-b4dd-cdab85362566",
   "metadata": {},
   "source": [
    "def calculate_fid(real_mean, real_cov, gen_mean, gen_cov):\n",
    "    \"\"\"Calculate the Fréchet Inception Distance (FID).\"\"\"\n",
    "    mean_diff = np.sum((real_mean - gen_mean) ** 2)\n",
    "    cov_sqrt = sqrtm(real_cov @ gen_cov)  # Matrix square root\n",
    "    # Handle numerical errors\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real\n",
    "    fid = mean_diff + np.trace(real_cov + gen_cov - 2 * cov_sqrt)\n",
    "    return fid\n",
    "\n",
    "# Compute FID\n",
    "fid_score = calculate_fid(real_mean, real_cov, gen_mean, gen_cov)\n",
    "print(f\"FID Score: {fid_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056525d-6dae-4dca-992a-2598406d1fe6",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "fid_score = calculate_fid(real_mean, real_cov, gen_mean, gen_cov)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"FID Score: {fid_score:.4f}\")\n",
    "print(f\"Time Taken: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a69ba-f8a8-45cf-84e0-02ed1b767cd8",
   "metadata": {},
   "source": [
    "### Interpret the FID Score\n",
    "Lower is Better: A lower FID score means the generated embeddings are closer to the real ones.\n",
    "\n",
    "Thresholds:\n",
    "\n",
    "FID ~10: High-quality generations.\n",
    "\n",
    "FID > 50: Poor-quality generations (distribution mismatch).\n",
    "\n",
    "Advantages of FID over MSE\n",
    "\n",
    "1. Captures Distributional Similarity: Unlike MSE, which evaluates individual points, FID compares entire distributions.\n",
    "2. Handles Diversity: FID penalizes lack of diversity in generated data.\n",
    "3. Task-Agnostic: FID works well for tasks where direct reconstruction is not the only goal (e.g., generating new domain configurations or embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb8c74-1ad3-48aa-bbc5-d328c3b48ae5",
   "metadata": {},
   "source": [
    "### Time Complexity\n",
    "\n",
    "#### How Much Time Will FID Calculation Take?\n",
    "\n",
    "The time required to calculate the **Fréchet Inception Distance (FID)** using the provided function depends on several factors:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Factors Influencing Time**\n",
    "1. **Dimensionality of Features (Number of Features)**:\n",
    "   - Higher-dimensional feature spaces (e.g., thousands of features) will take more time because matrix operations (like covariance computation and square root calculation) become more computationally expensive.\n",
    "\n",
    "2. **Matrix Square Root (`sqrtm`)**:\n",
    "   - This is the most computationally expensive operation in the function. It has a complexity of \\(O(d^3)\\), where \\(d\\) is the dimensionality of the covariance matrices.\n",
    "\n",
    "3. **Hardware**:\n",
    "   - The CPU or GPU you are using significantly impacts the computation time.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Time Complexity**\n",
    "Assuming \\(d\\) is the dimensionality of the embeddings (e.g., 2048 for Inception-based features):\n",
    "- **Matrix Square Root (`sqrtm`)**: \\(O(d^3)\\)\n",
    "- **Matrix Multiplication (`real_cov @ gen_cov`)**: \\(O(d^3)\\)\n",
    "- **Trace Calculation and Mean Squared Error**: \\(O(d^2)\\)\n",
    "\n",
    "Thus, the overall time complexity is approximately **\\(O(d^3)\\)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Empirical Estimation**\n",
    "- For embeddings of size \\(d = 2048\\), typical in Inception-based features:\n",
    "  - **Small Datasets (e.g., 10,000 samples)**: A few seconds to a minute on a modern CPU.\n",
    "  - **Larger Datasets**: Time increases proportionally with the number of samples due to covariance computation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Optimizing the Calculation**\n",
    "1. **Use GPU Acceleration**:\n",
    "   - Libraries like PyTorch or TensorFlow can handle matrix operations on GPUs more efficiently.\n",
    "\n",
    "2. **Precompute Covariance**:\n",
    "   - If you're comparing multiple generated sets to the same real data, precomputing the real data statistics (mean and covariance) can save time.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Benchmarking FID Calculation**\n",
    "You can measure the time taken to compute FID in your environment using Python’s `time` module:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "fid_score = calculate_fid(real_mean, real_cov, gen_mean, gen_cov)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"FID Score: {fid_score:.4f}\")\n",
    "print(f\"Time Taken: {end_time - start_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
